{"cells":[{"cell_type":"code","source":["pip install --upgrade numpy"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"261936a7-a034-40e3-a577-207b8687470d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting numpy\n  Using cached numpy-1.22.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.2\n    Uninstalling numpy-1.21.2:\n      Successfully uninstalled numpy-1.21.2\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\npetastorm 0.10.0 requires pyspark&gt;=2.1.0, which is not installed.\ntensorflow-cpu 2.4.1 requires numpy~=1.19.2, but you&#39;ll have numpy 1.22.1 which is incompatible.\npandas-profiling 2.11.0 requires seaborn&gt;=0.10.1, but you&#39;ll have seaborn 0.10.0 which is incompatible.\nkoalas 1.8.0 requires numpy&lt;1.20.0,&gt;=1.14, but you&#39;ll have numpy 1.22.1 which is incompatible.\nSuccessfully installed numpy-1.22.1\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting numpy\n  Using cached numpy-1.22.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.2\n    Uninstalling numpy-1.21.2:\n      Successfully uninstalled numpy-1.21.2\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\npetastorm 0.10.0 requires pyspark&gt;=2.1.0, which is not installed.\ntensorflow-cpu 2.4.1 requires numpy~=1.19.2, but you&#39;ll have numpy 1.22.1 which is incompatible.\npandas-profiling 2.11.0 requires seaborn&gt;=0.10.1, but you&#39;ll have seaborn 0.10.0 which is incompatible.\nkoalas 1.8.0 requires numpy&lt;1.20.0,&gt;=1.14, but you&#39;ll have numpy 1.22.1 which is incompatible.\nSuccessfully installed numpy-1.22.1\nPython interpreter will be restarted.\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import csv\nimport json\nimport random\nimport requests\nimport sys\nimport time\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\npd.set_option(\"display.max_rows\", 100)\nimport numpy as np\nfrom tabulate import tabulate\nfrom pyspark.sql import SparkSession\nfrom datetime import datetime\n\n#feature functions\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nimport textdistance\nfrom fuzzywuzzy import fuzz\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom math import log2\nimport sparse_dot_topn.sparse_dot_topn as ct\nfrom scipy.sparse import csr_matrix\n\nfrom __future__ import print_function\nimport swagger_client\nfrom swagger_client.rest import ApiException\nfrom pprint import pprint"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06dd1b01-9bf8-428f-9e4c-e455e8c66edf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-1d1e5d2e-d587-457d-95ee-83e190e71def/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn(&#39;Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning&#39;)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-1d1e5d2e-d587-457d-95ee-83e190e71def/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn(&#39;Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning&#39;)\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#Helper Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df6ce000-a262-4007-81a9-8947c529d19a"}}},{"cell_type":"code","source":["def company_run_match(df, company_name_field='receiver_name', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[company_name_field]\n    company_match_url =f'https://{endpoint}.altana.ai/atlas/v1/company/match/company_name={company_name}'\n    resp = requests.get(company_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame([resp.json()])\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\ndef company_run_search(df, company_name_field='receiver_name', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[company_name_field]\n    company_match_url =f'https://{endpoint}.altana.ai/atlas/v1/company/search/company_name={company_name}'\n    resp = requests.get(company_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame(resp.json()['companies']).head(30)\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n#---------------------------------------------------------------------------------------------------------------------------------------------------\n\ndef facility_run_match(df, facility_name_field='receiver_name', address_string_field='receiver_full_address', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[facility_name_field]\n    address_str = row[address_string_field]\n    facility_match_url =f'https://{endpoint}.altana.ai/atlas/v1/facility/match?company_name={company_name}&full_address={address_str}'\n    resp = requests.get(facility_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame([resp.json()])\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\n\ndef facility_run_search(df, facility_name_field='receiver_name', address_string_field='receiver_full_address', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[facility_name_field]\n    address_str = row[address_string_field]\n    facility_match_url =f'https://{endpoint}.altana.ai/atlas/v1/facility/search?company_name={company_name}&full_address={address_str}'\n    resp = requests.get(facility_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame(resp.json()['facilities']).head(30)\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\n\ndef address_filter(df, min_geocode_level=18, street_level_match=True, min_geo_confidence=0.4):\n  df_filtered = df[(df['address_model_output_level']=='geocode_str')]\n  \n  out_df = pd.concat([df_filtered.reset_index(drop=True),pd.DataFrame(list(df_filtered['geo_string_address_model_metadata']))], axis=1)\n  out_df = pd.concat([out_df, pd.DataFrame(list(df_filtered['geocoder_metadata']))], axis=1)\n  \n  tdf = pd.DataFrame(list(out_df['layer_properties_in']))\n  cols = []\n  for c in tdf.columns:\n    cols.append(c +'_in')\n  tdf.columns = cols\n  out_df = pd.concat([out_df, tdf], axis=1)\n  \n  tdf = pd.DataFrame(list(out_df['layer_properties_out']))\n  cols = []\n  for c in tdf.columns:\n    cols.append(c +'_out')\n  tdf.columns = cols\n  out_df = pd.concat([out_df, tdf], axis=1)\n  \n  if street_level_match:\n    out_df = out_df[out_df['street_in']==out_df['street_out']]\n    if (out_df['housenumber_in'] is not None  & out_df['housenumber_out'] is not None):\n      out_df = out_df[out_df['housenumber_in']==out_df['housenumber_out']]\n\n  out_df = out_df[out_df['geo_confidence_in'] > min_geo_confidence]\n  out_df = out_df[out_df['geo_confidence_out'] > min_geo_confidence]\n  \n  out_df = out_df[out_df['geo_level_in'] > min_geocode_level]\n  out_df = out_df[out_df['geo_level_out'] > min_geocode_level]\n  \n  return out_df\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\ndef awesome_cossim_top(A:csr_matrix, B:csr_matrix, ntop:int, lower_bound:float=0)->csr_matrix:\n  '''\n  Runs optimizes cosine similarity on two sparse matrices\n  \n  Parameters\n  ----------\n  A: \"dirty\" - companies we are trying to weed out \n  B: \"clean\" - company name (query) search\n  ntop: stores ntop similar items (if compared against several strings)\n  lower_bound: stores items with a similarity above lower_bound (if compared against several strings)\n  \n  Output\n  ----------\n  matches_sim: cosine similarity based on the TFIDF of an n-grams, metric between 0 and 1\n  '''\n  A = A.tocsr()\n  B = B.tocsr()\n  M, _ = A.shape\n  _, N = B.shape\n\n  idx_dtype = np.int32\n\n  nnz_max = M * ntop\n\n  indptr = np.zeros(M + 1, dtype=idx_dtype)\n  indices = np.zeros(nnz_max, dtype=idx_dtype)\n  data = np.zeros(nnz_max, dtype=A.dtype)\n\n  ct.sparse_dot_topn(\n      M, N, np.asarray(A.indptr, dtype=idx_dtype),\n      np.asarray(A.indices, dtype=idx_dtype),\n      A.data,\n      np.asarray(B.indptr, dtype=idx_dtype),\n      np.asarray(B.indices, dtype=idx_dtype),\n      B.data,\n      ntop,\n      lower_bound,\n      indptr, indices, data)\n  return csr_matrix((data, indices, indptr), shape=(M, N))\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\ndef find_similarity(query_name:str, canon_name:str, n_gram:int=3)-> float:\n  '''\n  Evaluates cosine similarity of two strings based on their n-gram TFIDF. Leverages sparse matrix for fast calculations.\n  \n  Parameters\n  ----------\n  query_name: company to search\n  canon_name: output of company names best matched from database (companies we want to narrow down)\n  n_gram: number of contiguous sequence of n characters, 3 is the default. Note: higher you go -> more precise matching and lower cosine similarity will be \n  \n  Output\n  ----------\n  matches_sim: cosine similarity based on the TFIDF of an n-grams, metric between 0 and 1\n  '''\n  def ngrams(string, n=n_gram):\n    #string = (re.sub(r'[,-./]',r'', string)).upper()\n    string = (re.sub(r'[^A-Za-z0-9]+',r'', string)).upper()\n    ngrams = zip(*[string[i:] for i in range(n)])\n    return [''.join(ngram) for ngram in ngrams]\n  \n  # constructs your vectorizer for building the TF-IDF matrix\n  vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n\n  # builds a sparse document term matrix of the query company name\n  tf_idf_matrix_clean = vectorizer.fit_transform([query_name]) # convert to dense matrix tf_idf_matrix_clean.todense()\n\n  # builds a sparse document term matrix of the canon company name\n  tf_idf_matrix_dirty = vectorizer.transform([canon_name])\n\n  # if there is completely no overlap between two strings, sparse matrix will be empty (bc dense matrix will have all zeros)\n  # as the result, awesome_cossim_top will throw an error, need if statement to catch that \n  if tf_idf_matrix_clean.size == 0 or tf_idf_matrix_dirty.size == 0:\n      matches_sim = [0]\n  else:\n      # runs optimizes cosine similarity on two sparse matrices\n      matches = awesome_cossim_top(tf_idf_matrix_dirty, tf_idf_matrix_clean.transpose(), 1, 0.0)\n      # unpacks results from matches \n      #matches_sim = get_matches_df(matches, tf_idf_matrix_dirty, tf_idf_matrix_clean, top=0)\n      \n      # convert sparse similarty to dense similarity\n      matches_sim = matches.data\n    \n  return matches_sim[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d6e2842-0acb-4155-b11e-98d51e92489f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Search Match Helper Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3910902059778579}},"nbformat":4,"nbformat_minor":0}
