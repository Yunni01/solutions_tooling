{"cells":[{"cell_type":"markdown","source":["##### Modify Variables in the Widgets at the Top of the Screen\n* **Address Column** - The column name where the addresses are stored in the uploaded file\n* **Company Name** - The column name where the company names are stored in the uploaded BOM file\n* **Input CSV Company Search** - The file path you uploaded your file for company search and match\n* **Input CSV Facility Search** - The file path you uploaded your file for facility search and match (needs company/supplier name and address column)\n* **Supplier Name** - The column name where the supplier names are stored in uploaded file \n* **Requiremt for Faciltiy Search/Match** - Address Column, Input CSV Facility Search OR table name, and Supplier Name\n* **Requiremt for Company Search/Match** - Company Name, Input CSV Company Search OR table name\n\n* **IF IMPORTING FROM Table** - Fill out the relevant Address Column, Company Name, and Supplier Name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db3f0694-bbd7-4c7c-987d-d245f11487bc"}}},{"cell_type":"code","source":["company_name = dbutils.widgets.get(\"Company Name\")\naddress_string = dbutils.widgets.get(\"Address Column\")\nfacility_name = dbutils.widgets.get(\"Supplier Name\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e8f8ec8-a8c7-4fb0-8ece-ffe26f837210"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Install & Import Packages"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2083ea70-5cfe-41ce-8ee6-8a6d24aa07f5"}}},{"cell_type":"code","source":["pip install --upgrade numpy"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7945bd79-aaf7-49a1-ba86-49a77802dbb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting numpy\n  Using cached numpy-1.22.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.2\n    Uninstalling numpy-1.21.2:\n      Successfully uninstalled numpy-1.21.2\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\npetastorm 0.10.0 requires pyspark&gt;=2.1.0, which is not installed.\ntensorflow-cpu 2.4.1 requires numpy~=1.19.2, but you&#39;ll have numpy 1.22.1 which is incompatible.\npandas-profiling 2.11.0 requires seaborn&gt;=0.10.1, but you&#39;ll have seaborn 0.10.0 which is incompatible.\nkoalas 1.8.0 requires numpy&lt;1.20.0,&gt;=1.14, but you&#39;ll have numpy 1.22.1 which is incompatible.\nSuccessfully installed numpy-1.22.1\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting numpy\n  Using cached numpy-1.22.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.2\n    Uninstalling numpy-1.21.2:\n      Successfully uninstalled numpy-1.21.2\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\npetastorm 0.10.0 requires pyspark&gt;=2.1.0, which is not installed.\ntensorflow-cpu 2.4.1 requires numpy~=1.19.2, but you&#39;ll have numpy 1.22.1 which is incompatible.\npandas-profiling 2.11.0 requires seaborn&gt;=0.10.1, but you&#39;ll have seaborn 0.10.0 which is incompatible.\nkoalas 1.8.0 requires numpy&lt;1.20.0,&gt;=1.14, but you&#39;ll have numpy 1.22.1 which is incompatible.\nSuccessfully installed numpy-1.22.1\nPython interpreter will be restarted.\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["try:\n  __import__(\"postal\")\nexcept ImportError:\n  # Use Conda to install LibPostal\n  ! conda install -c conda-forge libpostal postal\ntry:\n  __import__(\"swagger_client\")\nexcept ImportError:\n  ! pip install git+https://github.com/altana-tech/atlas-api-1.0.71-python-sdk.git\n\nimport csv\nimport json\nimport random\nimport requests\nimport sys\nimport time\nimport pandas as pd\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\npd.set_option(\"display.max_rows\", 100)\nimport numpy as np\nfrom tabulate import tabulate\nfrom pyspark.sql import SparkSession\nfrom datetime import datetime\n\n#feature functions\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nimport textdistance\nfrom fuzzywuzzy import fuzz\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom math import log2\nimport sparse_dot_topn.sparse_dot_topn as ct\nfrom scipy.sparse import csr_matrix\n\nfrom __future__ import print_function\nimport swagger_client\nfrom swagger_client.rest import ApiException\nfrom pprint import pprint\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"006c3c85-ef36-40ff-be78-aaa88709cc9d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-887b7cdd-63ef-4071-af25-ee23c0a3272f/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn(&#39;Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning&#39;)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-887b7cdd-63ef-4071-af25-ee23c0a3272f/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn(&#39;Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning&#39;)\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Set API Token & Parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"467f6253-7d4a-46c4-b1b1-6b5ae0e5798c"}}},{"cell_type":"code","source":["API_TOKEN = 'MTpZdW5uaToxNjM5NDIwODQxOmI0MThmZjdm.ZjI4MTliYWQyZTVmYzIzMTAwNTUyZTg1MzkxMDZkZTM0NjFmMDM0Nw'\napi_headers = {'X-Api-Key':API_TOKEN,\n               'Accept':'application/json'}\n\n#Change endpoint parameter to be either 'api' or 'api-staging'\nendpoint = 'api-staging' #'api'\n\n#Set timestamp for saved CSV file \nfiletimestamp = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e28ed4e6-cf9f-4693-bcf7-ce26b0f94434"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Helper Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30d0adea-861f-4c7f-ab16-62a97487c1e2"}}},{"cell_type":"code","source":["def company_run_match(df, company_name_field='receiver_name', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[company_name_field]\n    company_match_url =f'https://{endpoint}.altana.ai/atlas/v1/company/match/company_name={company_name}'\n    resp = requests.get(company_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame([resp.json()])\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\ndef company_run_search(df, company_name_field='receiver_name', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[company_name_field]\n    company_match_url =f'https://{endpoint}.altana.ai/atlas/v1/company/search/company_name={company_name}'\n    resp = requests.get(company_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame(resp.json()['companies']).head(30)\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n#---------------------------------------------------------------------------------------------------------------------------------------------------\n\ndef facility_run_match(df, facility_name_field='receiver_name', address_string_field='receiver_full_address', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[facility_name_field]\n    address_str = row[address_string_field]\n    facility_match_url =f'https://{endpoint}.altana.ai/atlas/v1/facility/match?company_name={company_name}&full_address={address_str}'\n    resp = requests.get(facility_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame([resp.json()])\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\n\ndef facility_run_search(df, facility_name_field='receiver_name', address_string_field='receiver_full_address', row_id='transaction_id'):\n  errors = []\n  df = df.reset_index(drop=True)\n  start_time = pd.Timestamp.now()\n  match_results = []\n  for ix, row in df.iterrows():\n    if ix%100 == 0:\n      print(ix)\n    company_name = row[facility_name_field]\n    address_str = row[address_string_field]\n    facility_match_url =f'https://{endpoint}.altana.ai/atlas/v1/facility/search?company_name={company_name}&full_address={address_str}'\n    resp = requests.get(facility_match_url, headers=api_headers)\n    if resp.status_code!=200:\n      errors.append(resp)\n    else:\n      result_df = pd.DataFrame(resp.json()['facilities']).head(30)\n      result_df[row_id] = row[row_id]\n      match_results.append(result_df)\n  end_time = pd.Timestamp.now()\n\n  out_df = pd.merge(df, pd.concat(match_results), on=row_id, how='left', indicator=True)\n  avg_sec = ((end_time-start_time).total_seconds())/len(df)\n  print(f'avg api response time in seconds: {avg_sec}')\n  return out_df, errors\n\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\n\ndef address_filter(df, min_geocode_level=18, street_level_match=True, min_geo_confidence=0.4):\n  df_filtered = df[(df['address_model_output_level']=='geocode_str')]\n  \n  out_df = pd.concat([df_filtered.reset_index(drop=True),pd.DataFrame(list(df_filtered['geo_string_address_model_metadata']))], axis=1)\n  out_df = pd.concat([out_df, pd.DataFrame(list(df_filtered['geocoder_metadata']))], axis=1)\n  \n  tdf = pd.DataFrame(list(out_df['layer_properties_in']))\n  cols = []\n  for c in tdf.columns:\n    cols.append(c +'_in')\n  tdf.columns = cols\n  out_df = pd.concat([out_df, tdf], axis=1)\n  \n  tdf = pd.DataFrame(list(out_df['layer_properties_out']))\n  cols = []\n  for c in tdf.columns:\n    cols.append(c +'_out')\n  tdf.columns = cols\n  out_df = pd.concat([out_df, tdf], axis=1)\n  \n  if street_level_match:\n    out_df = out_df[out_df['street_in']==out_df['street_out']]\n    if (out_df['housenumber_in'] is not None  & out_df['housenumber_out'] is not None):\n      out_df = out_df[out_df['housenumber_in']==out_df['housenumber_out']]\n\n  out_df = out_df[out_df['geo_confidence_in'] > min_geo_confidence]\n  out_df = out_df[out_df['geo_confidence_out'] > min_geo_confidence]\n  \n  out_df = out_df[out_df['geo_level_in'] > min_geocode_level]\n  out_df = out_df[out_df['geo_level_out'] > min_geocode_level]\n  \n  return out_df\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\ndef awesome_cossim_top(A:csr_matrix, B:csr_matrix, ntop:int, lower_bound:float=0)->csr_matrix:\n  '''\n  Runs optimizes cosine similarity on two sparse matrices\n  \n  Parameters\n  ----------\n  A: \"dirty\" - companies we are trying to weed out \n  B: \"clean\" - company name (query) search\n  ntop: stores ntop similar items (if compared against several strings)\n  lower_bound: stores items with a similarity above lower_bound (if compared against several strings)\n  \n  Output\n  ----------\n  matches_sim: cosine similarity based on the TFIDF of an n-grams, metric between 0 and 1\n  '''\n  A = A.tocsr()\n  B = B.tocsr()\n  M, _ = A.shape\n  _, N = B.shape\n\n  idx_dtype = np.int32\n\n  nnz_max = M * ntop\n\n  indptr = np.zeros(M + 1, dtype=idx_dtype)\n  indices = np.zeros(nnz_max, dtype=idx_dtype)\n  data = np.zeros(nnz_max, dtype=A.dtype)\n\n  ct.sparse_dot_topn(\n      M, N, np.asarray(A.indptr, dtype=idx_dtype),\n      np.asarray(A.indices, dtype=idx_dtype),\n      A.data,\n      np.asarray(B.indptr, dtype=idx_dtype),\n      np.asarray(B.indices, dtype=idx_dtype),\n      B.data,\n      ntop,\n      lower_bound,\n      indptr, indices, data)\n  return csr_matrix((data, indices, indptr), shape=(M, N))\n\n#---------------------------------------------------------------------------------------------------------------------------------------------------\ndef find_similarity(query_name:str, canon_name:str, n_gram:int=3)-> float:\n  '''\n  Evaluates cosine similarity of two strings based on their n-gram TFIDF. Leverages sparse matrix for fast calculations.\n  \n  Parameters\n  ----------\n  query_name: company to search\n  canon_name: output of company names best matched from database (companies we want to narrow down)\n  n_gram: number of contiguous sequence of n characters, 3 is the default. Note: higher you go -> more precise matching and lower cosine similarity will be \n  \n  Output\n  ----------\n  matches_sim: cosine similarity based on the TFIDF of an n-grams, metric between 0 and 1\n  '''\n  def ngrams(string, n=n_gram):\n    #string = (re.sub(r'[,-./]',r'', string)).upper()\n    string = (re.sub(r'[^A-Za-z0-9]+',r'', string)).upper()\n    ngrams = zip(*[string[i:] for i in range(n)])\n    return [''.join(ngram) for ngram in ngrams]\n  \n  # constructs your vectorizer for building the TF-IDF matrix\n  vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n\n  # builds a sparse document term matrix of the query company name\n  tf_idf_matrix_clean = vectorizer.fit_transform([query_name]) # convert to dense matrix tf_idf_matrix_clean.todense()\n\n  # builds a sparse document term matrix of the canon company name\n  tf_idf_matrix_dirty = vectorizer.transform([canon_name])\n\n  # if there is completely no overlap between two strings, sparse matrix will be empty (bc dense matrix will have all zeros)\n  # as the result, awesome_cossim_top will throw an error, need if statement to catch that \n  if tf_idf_matrix_clean.size == 0 or tf_idf_matrix_dirty.size == 0:\n      matches_sim = [0]\n  else:\n      # runs optimizes cosine similarity on two sparse matrices\n      matches = awesome_cossim_top(tf_idf_matrix_dirty, tf_idf_matrix_clean.transpose(), 1, 0.0)\n      # unpacks results from matches \n      #matches_sim = get_matches_df(matches, tf_idf_matrix_dirty, tf_idf_matrix_clean, top=0)\n      \n      # convert sparse similarty to dense similarity\n      matches_sim = matches.data\n    \n  return matches_sim[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd201436-aa4b-4de4-b253-97e948d8595a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Load Data from Table Stored in Databricks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a31fe6a5-d09e-490f-b0f6-69759391a534"}}},{"cell_type":"code","source":["bsci_all_supplier = sqlContext.sql(\"SELECT * FROM bsci_v4.search_match_input_all_suppliers\")\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n\nbsci_all_supplier_df = bsci_all_supplier.toPandas()\n\ncompany_df = bsci_all_supplier_df\ncompany_df['custom_id'] = company_df.index\n\nfacility_df3 = bsci_all_supplier_df\nfacility_df3['custom_id'] = facility_df3.index"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99cf70c9-72b6-4301-8739-ac599b9f24a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Search by Facility \nSearches by supplier name & address"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"436884d5-fe0c-4fd5-82dd-02415b2c661e"}}},{"cell_type":"code","source":["facility_search_results, errors = facility_run_search(df=facility_df, \n                                    facility_name_field=facility_name,\n                                    address_string_field=address_string, \n                                    row_id='custom_id')\n\noutname_search = 'facility_search_results_dataframe_'+filetimestamp+'.csv'\noutdir_search = '/dbfs/FileStore/search_match_data/'\nfacility_search_results.to_csv(outdir_search+outname_search, index=False, encoding=\"utf-8\")\nprint(filetimestamp)\n\n#Loop through function to get similarity scores between supplier name and company name \nngram_list_facility = []\nfor i, j in zip(facility_search_results['Supplier Name'], facility_search_results['company_name']):\n  try: \n    sim_1 = find_similarity(i, j)\n    ngram_list_facility.append(sim_1)\n  except:\n        ngram_list_facility.append(0)\n  facility_search_results_scores = pd.DataFrame(ngram_list_facility)\n  facility_search_results_scores = facility_search_results_scores.rename({0: 'company_sim_score'}, axis=1)\n  facility_search_results_merged = facility_search_results.merge(facility_search_results_scores, how='outer', left_index=True, right_index=True)\nfacility_search_results_merged.head()\n\n#Save dataframe from facility search as table\nfacility_search_results_merged[facility_search_results_merged.columns] = facility_search_results_merged[facility_search_results_merged.columns].astype(str)\n\n#Convert resulting pandas dataframe to spark dataframe and then save as table\nfacility_search_results_merged_df_spark = spark.createDataFrame(facility_search_results_merged)\nfacility_search_results_merged_df_spark.write.mode(\"overwrite\").saveAsTable(\"bsci_v4.atlas_bulk_facility_search\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f704a6e-c90a-4799-9231-fff394f2846a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 1.5538524910858995\n2022_01_18-07_50_41_PM\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 1.5538524910858995\n2022_01_18-07_50_41_PM\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["# Filtered Facility Search by Geo Confidence\nChange min_geo_confidence level to subset on the search results"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c78d43d5-e58f-4d2e-950c-7a8babc7a7e0"}}},{"cell_type":"code","source":["#Filtered search results for facilities\n#Can change min_geo_confidence and min_geocode_level\nfiltered_search_results = address_filter(facility_search_results, min_geo_confidence=0.4, min_geocode_level=16, street_level_match=False)\nfiltered_search_results.head()\n\n\n#Save dataframe from facility search as table\nfiltered_search_results[filtered_search_results.columns] = filtered_search_results[filtered_search_results.columns].astype(str)\n\n#Convert resulting pandas dataframe to spark dataframe and then save as table\nfiltered_search_results_df_spark = spark.createDataFrame(filtered_search_results)\nfiltered_search_results_df_spark.write.mode(\"overwrite\").saveAsTable(\"bsci_v4.atlas_bulk_facility_search_geo_filtered\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d29cb018-9161-4b41-80c2-0fb974f9fa17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">2022_01_18-07_50_41_PM\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2022_01_18-07_50_41_PM\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#Match by Facility\nMatch by supplier name & address"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25dcdac3-b44d-43e0-b635-65867cf01a67"}}},{"cell_type":"code","source":["#Parameterize the input fields and api/api-staing in functions \nfacility_match_results, errors = facility_run_match(df=facility_df, \n                                    facility_name_field= facility_name,\n                                    address_string_field=address_string, \n                                    row_id='custom_id')\n\n#Save dataframe from facility match as table\nfacility_match_results[facility_match_results.columns] = facility_match_results[facility_match_results.columns].astype(str)\n\n#Convert resulting pandas dataframe to spark dataframe and then save as table\nfacility_match_results_df_spark = spark.createDataFrame(facility_match_results)\nfacility_match_results_df_spark.write.mode(\"overwrite\").saveAsTable(\"bsci_v4.atlas_bulk_facility_match\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5343e1dd-a46f-44ac-b82a-35d521db8687"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 1.52253771636953\n2022_01_18-07_50_41_PM\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 1.52253771636953\n2022_01_18-07_50_41_PM\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#Search by Company\nSearch by company name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b07c7095-ece4-43ac-b03c-bfd90289d64f"}}},{"cell_type":"code","source":["company_search_results, errors = company_run_search(df=company_df, \n                                    company_name_field=company_name, \n                                    row_id='custom_id')\n\nngram_list = []\nfor i, j in zip(company_search_results['Supplier Name'], company_search_results['company_name']):\n  try: \n    sim_1 = find_similarity(i, j)\n    ngram_list.append(sim_1)\n  except:\n        ngram_list.append(0)\n  company_search_results_scores = pd.DataFrame(ngram_list)\n  company_search_results_scores = company_search_results_scores.rename({0: 'company_sim_score'}, axis=1)\n  company_search_results_merged = company_search_results.merge(company_search_results_scores, how='outer', left_index=True, right_index=True)\n\n\n#Save dataframe from company search as table\ncompany_search_results_merged[company_search_results_merged.columns] = company_search_results_merged[company_search_results_merged.columns].astype(str)\n\n#Convert resulting pandas dataframe to spark dataframe and then save as table\ncompany_search_results_merged_df_spark = spark.createDataFrame(company_search_results_merged)\ncompany_search_results_merged_df_spark.write.mode(\"overwrite\").saveAsTable(\"bsci_v4.atlas_bulk_company_search\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d28df6ad-df38-4060-87a3-65b379816828"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 0.49674764019448947\n2022_01_18-07_50_41_PM\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 0.49674764019448947\n2022_01_18-07_50_41_PM\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#Match by Company\nMatch by company name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa049a68-1fc3-46a4-8517-8dd16328c17d"}}},{"cell_type":"code","source":["#Check to see if the company cosine similarity is avaiable from the company api \n#which rank it falls in search rank (number of transactions with which facility)\n#in top 10 search results, creating more filitered search results \ncompany_match_results, errors = company_run_match(df=company_df, \n                                    company_name_field=company_name, \n                                    row_id='custom_id')\n\n#Save dataframe from company match as table\ncompany_match_results[company_match_results.columns] = company_match_results[company_match_results.columns].astype(str)\n\n#Convert resulting pandas dataframe to spark dataframe and then save as table\ncompany_match_results_df_spark = spark.createDataFrame(company_match_results)\ncompany_match_results_df_spark.write.mode(\"overwrite\").saveAsTable(\"bsci_v4.atlas_bulk_company_match\")\n\n#See Cmd 52 for download instructions "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"691890c5-8d62-4a06-8cc7-86acd0e5ea58"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 0.5896858962722853\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0\n100\n200\n300\n400\n500\n600\navg api response time in seconds: 0.5896858962722853\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["###End of Code"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1ddf92f-864e-4a7c-816a-25618b03c490"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"search_match_using_db_table","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"Supplier Name":{"nuid":"9ef3b48b-6157-4aec-ada1-634a8a09e16c","currentValue":"Supplier Name","widgetInfo":{"widgetType":"text","name":"Supplier Name","defaultValue":" ","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Company Name":{"nuid":"dfc20878-79c3-42bd-81bf-c5e6b6061125","currentValue":"Supplier Name","widgetInfo":{"widgetType":"text","name":"Company Name","defaultValue":" ","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Address Column":{"nuid":"56806e4e-66a4-4037-866f-c23b9897d4d0","currentValue":"Address","widgetInfo":{"widgetType":"text","name":"Address Column","defaultValue":" ","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":3910902059779205}},"nbformat":4,"nbformat_minor":0}
